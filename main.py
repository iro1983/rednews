import os
import json
import requests
import praw
from pytrends.request import TrendReq
from datetime import datetime

# ================= 1. è·å–ç¯å¢ƒé…ç½® =================
REDDIT_ID = os.environ.get("REDDIT_CLIENT_ID")
REDDIT_SECRET = os.environ.get("REDDIT_SECRET")
MINIMAX_KEY = os.environ.get("MINIMAX_API_KEY")
MINIMAX_GROUP_ID = os.environ.get("MINIMAX_GROUP_ID")

# ================= 2. æŠ“å–æ•°æ® =================
def get_data():
    data_summary = {"date": datetime.now().strftime("%Y-%m-%d"), "trends": []}
    
    # --- Reddit ---
    try:
        if REDDIT_ID and REDDIT_SECRET:
            reddit = praw.Reddit(client_id=REDDIT_ID, client_secret=REDDIT_SECRET, user_agent='news_bot/1.0')
            posts = []
            for sub in ["news", "popular"]:
                for post in reddit.subreddit(sub).hot(limit=5):
                    posts.append(f"[{sub}] {post.title} (Score: {post.score})")
            data_summary["reddit"] = posts
        else:
            data_summary["reddit"] = ["æœªé…ç½® Reddit APIï¼Œè·³è¿‡æŠ“å–"]
    except Exception as e:
        data_summary["reddit"] = [f"æŠ“å–å‡ºé”™: {str(e)}"]

    # --- Google Trends ---
    try:
        pytrends = TrendReq(hl='en-US', tz=360)
        trends = pytrends.trending_searches(pn='united_states').head(10)[0].tolist()
        data_summary["google"] = trends
    except Exception as e:
        data_summary["google"] = [f"æŠ“å–å‡ºé”™: {str(e)}"]

    return data_summary

# ================= 3. è°ƒç”¨ Minimax AI =================
def analyze_with_minimax(data):
    if not MINIMAX_KEY or not MINIMAX_GROUP_ID:
        return "âŒ é”™è¯¯ï¼šæœªé…ç½® Minimax API Keyï¼Œæ— æ³•ç”Ÿæˆåˆ†æã€‚è¯·æŸ¥çœ‹ raw_data.json æ‰‹åŠ¨åˆ†æã€‚"

    url = f"https://api.minimax.chat/v1/text/chatcompletion_pro?GroupId={MINIMAX_GROUP_ID}"
    headers = {"Authorization": f"Bearer {MINIMAX_KEY}", "Content-Type": "application/json"}
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªç¾å›½æ–°é—»åˆ†æå¸ˆã€‚ä»Šå¤©æ˜¯ {data['date']}ã€‚
    è¯·æ ¹æ®ä»¥ä¸‹é‡‡é›†åˆ°çš„ Google çƒ­æœå’Œ Reddit çƒ­å¸–ï¼Œå†™ä¸€ä»½ã€Šç¾å›½ä»Šæ—¥çƒ­ç‚¹ç®€æŠ¥ã€‹ã€‚
    è¦æ±‚ï¼š
    1. æ‰¾å‡ºæœ€çƒ­é—¨çš„3-5ä¸ªäº‹ä»¶ã€‚
    2. ç”¨ä¸­æ–‡æ€»ç»“ï¼Œæ¯ä¸ªäº‹ä»¶åŒ…å«ã€äº‹ä»¶æ¦‚è¿°ã€‘å’Œã€èˆ†è®ºååº”ã€‘ã€‚
    3. å¿½ç•¥æ— å…³çš„å¹¿å‘Šæˆ–æ— æ„ä¹‰å†…å®¹ã€‚
    
    æ•°æ®å¦‚ä¸‹ï¼š
    Googleçƒ­æœ: {json.dumps(data['google'], ensure_ascii=False)}
    Redditè®¨è®º: {json.dumps(data['reddit'], ensure_ascii=False)}
    """

    payload = {
        "model": "abab5.5-chat", # å¦‚æœä½ æœ‰abab6.5æƒé™å¯ä¿®æ”¹æ­¤å¤„
        "messages": [{"sender_type": "USER", "sender_name": "ç”¨æˆ·", "text": prompt}],
        "reply_constraints": {"sender_type": "BOT", "sender_name": "AIåŠ©æ‰‹"},
        "bot_setting": [{"bot_name": "AIåŠ©æ‰‹", "content": "ä½ æ˜¯ä¸€ä¸ªåˆ†æå¸ˆ"}]
    }
    
    try:
        res = requests.post(url, headers=headers, json=payload).json()
        return res['reply']
    except Exception as e:
        return f"AI è°ƒç”¨å¤±è´¥: {str(e)}"

# ================= ä¸»ç¨‹åº =================
if __name__ == "__main__":
    print("å¼€å§‹æŠ“å–...")
    data = get_data()
    
    # ä¿å­˜åŸå§‹æ•°æ®å¤‡ä»½
    with open("raw_data.json", "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
        
    print("å¼€å§‹AIåˆ†æ...")
    report = analyze_with_minimax(data)
    
    # å°†æŠ¥å‘Šå†™å…¥ Markdown æ–‡ä»¶
    with open("DAILY_REPORT.md", "w", encoding="utf-8") as f:
        f.write(f"# ğŸ‡ºğŸ‡¸ ç¾å›½çƒ­ç‚¹æ—¥æŠ¥ ({data['date']})\n\n")
        f.write(report)
        f.write("\n\n---\n*Report generated by GitHub Actions*")
    
    print("å®Œæˆï¼è¯·æŸ¥çœ‹ DAILY_REPORT.md")
